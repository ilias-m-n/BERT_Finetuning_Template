{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP3hjEY+RACi/Hod/unK24C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Package Installs for Google Colab\n","\n","Note: Google Colab comes with a great number of preinstalled packages. Therefore, here we only need to install a small number of specific packages. When setting up a python virtual-environment on a local machine, more installs will have to be performed."],"metadata":{"id":"2sdEhLYTH_4_"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets\n","!pip install accelerate -U\n","!pip install evaluate\n","!pip install \"ray[tune]\"\n","!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uw3jXtW-c4l5","executionInfo":{"status":"ok","timestamp":1703268252529,"user_tz":-60,"elapsed":52063,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}},"outputId":"861f03ef-901e-4c2d-bc58-f88374c2fc70"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.16.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Requirement already satisfied: ray[tune] in /usr/local/lib/python3.10/dist-packages (2.9.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.13.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (23.2)\n","Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n","Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n","Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.5.3)\n","Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.6.2.2)\n","Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (10.0.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]) (1.23.5)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.11.2)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.32.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.15.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2023.11.17)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n","Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.5.0)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.0)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"]}]},{"cell_type":"markdown","source":["# Package Imports"],"metadata":{"id":"XoiMafe4IZH2"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"BzaTHUtWcb4T","executionInfo":{"status":"ok","timestamp":1703268266732,"user_tz":-60,"elapsed":14205,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"outputs":[],"source":["import numpy as np\n","\n","\n","from datasets import load_dataset, load_metric\n","import evaluate\n","from transformers import (\n","     AutoTokenizer,\n","     DataCollatorWithPadding,\n","     TrainingArguments,\n","     AutoModelForSequenceClassification,\n","     Trainer,\n","     logging,\n","     AdamW,\n","     get_scheduler,\n","\n",")\n","import torch\n","from ray import tune, train\n","\n","# turn off warnings\n","#logging.set_verbosity_error()"]},{"cell_type":"markdown","source":["# Meta Variables\n","- base model\n","- loss function\n","- evaluation metrics\n","- best model metric\n","- number of trials\n"],"metadata":{"id":"IrZ91MToIwEf"}},{"cell_type":"code","source":["\"\"\"\n","Directory Paths:\n","\"\"\"\n","_path_initial_training = \"./initial_training\"\n","_path_secondary_training = \"./secondary_training\"\n","\n","\"\"\"\n","Base BERT model to be used during finetuning.\n","This has to be picked from the pre-trained models on HuggingFace\n","in order to be compatible with the Trainer API\n","\"\"\"\n","_base_model = \"bert-base-uncased\"\n","\n","\"\"\"\n","Three custom loss functions have been implemented:\n","  f1: soft-f1 macro score\n","  mcc: soft-mcc\n","  wce: weighted cross entropy\n","  ce: standard cross entropy\n","\"\"\"\n","_loss_fct = \"mcc\"\n","\n","\"\"\"\n","Metrics listed during evaluation:\n","\n","Note: adjust with desired metrics.\n","\"\"\"\n","_eval_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"matthews_correlation\"]\n","\n","\"\"\"\n","Specify which metric should be maximized during hyperparameter-search\n","Options:\n","- eval_matthews_correlation\n","- eval_f1\n","- eval_loss\n","- any other metric passed to the compute_metrics function\n","\"\"\"\n","_metric_best_model = \"eval_matthews_correlation\"\n","\n","\"\"\"\n","Number of trials to run during hyperparameter search.\n","\"\"\"\n","_no_trials = 4\n","\n","\"\"\"\n","Employ freezing of layers, options:\n","\"unfrozen\": all layers unfrozen\n","\"frozen\": all transformer layers frozen\n","\"\"\"\n","_frozen = \"unfrozen\""],"metadata":{"id":"iGkjtCJ6yAgG","executionInfo":{"status":"ok","timestamp":1703268266732,"user_tz":-60,"elapsed":28,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Setup\n","\n","This part has to be adjusted to whatever dataset and format used.\n","\n","Note: DataCollatorWithPadding allows for dynamic padding for individual batches. Only use with GPUs. For TPUs, use max_length padding attribute with Tokenizer instance."],"metadata":{"id":"hYAAghnjIzBt"}},{"cell_type":"markdown","source":["## Load Data\n","\n","-Note: We use GLUE's Microsoft Research Paraphrase Corpus for testing the functionality of this template\n","\n","https://huggingface.co/datasets/viewer/?dataset=glue&config=mrpc\n","\n","Binary Classification Task:\n","MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."],"metadata":{"id":"YVotGcMsQLKC"}},{"cell_type":"code","source":["raw_datasets = load_dataset(\"glue\", \"mrpc\")"],"metadata":{"id":"dnPxVcJqQLSC","executionInfo":{"status":"ok","timestamp":1703268268360,"user_tz":-60,"elapsed":1655,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Load Tokenizer"],"metadata":{"id":"dY2CaIW5QCY3"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(_base_model)"],"metadata":{"id":"w7qbphxLQAia","executionInfo":{"status":"ok","timestamp":1703268268723,"user_tz":-60,"elapsed":368,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Function that returns the Tokenizer so that we can employ data mapping.\n","\n","Note: Adjust this to desired task."],"metadata":{"id":"1MUIwuMIQ0Hx"}},{"cell_type":"code","source":["def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"],"metadata":{"id":"6F83g3QqQAo3","executionInfo":{"status":"ok","timestamp":1703268268724,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Map Dataset with Tokenizer"],"metadata":{"id":"EwQnmO5sRKgG"}},{"cell_type":"code","source":["tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"],"metadata":{"id":"QKBC_DDhc0sY","executionInfo":{"status":"ok","timestamp":1703268268724,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Instantiate DataCollator"],"metadata":{"id":"CvgXUeGJRZEg"}},{"cell_type":"code","source":["data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"LWJoWdWJRZMe","executionInfo":{"status":"ok","timestamp":1703268268724,"user_tz":-60,"elapsed":13,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Training Arguments\n","\n","Adjust to desired behaviour. Most arguments can be learned during hyperparameter-search."],"metadata":{"id":"WJWBvuj_JL8y"}},{"cell_type":"code","source":["\"\"\"\n","Create instance of class TrainingArguments. Adjust to desired behaviour.\n","\"\"\"\n","training_args = TrainingArguments(\n","    output_dir = _path_initial_training,\n","    # This was set for testing, when using template I would recommend adding\n","    # some sort of datatime argument to above meta path variables, to not loose\n","    # previous learnings.\n","    overwrite_output_dir = True,\n","    save_strategy = \"epoch\",\n","    evaluation_strategy = \"epoch\",\n","    logging_strategy = \"epoch\",\n","    metric_for_best_model = _metric_best_model,\n","    )"],"metadata":{"id":"O9t82JVzjz4j","executionInfo":{"status":"ok","timestamp":1703268282181,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Model Initialzation"],"metadata":{"id":"tjtjOcw3JQ-E"}},{"cell_type":"code","source":["\"\"\"\n","Here we supply two model init functions, one that freezes all encoder layers and\n","one that does not.\n","\n","Pass desired init function to Trainer below.\n","\n","Gradual unfreezing helps to strike a balance between leveraging pre-trained\n","knowledge and adapting to task-specific data. By unfreezing layers gradually\n","during training, the model learns to prioritize retaining general linguistic\n","knowledge in the early layers while fine-tuning the higher layers to adapt to\n","task-specific nuances. This mitigates overfitting by allowing the model to\n","gradually specialize on the new task without abruptly forgetting the\n","linguistic representations learned during pre-training, resulting in more\n","effective adaptation and improved generalization to the target task.\n","\n","Note: When utilizing gradual unfreezing you will have to train the model in\n","multiple steps. Gradually unfreezing ever more layers during training.\n","You will observe slower convergence, as such this will take more time.\n","\n","Note: Depending on the choice of a base model and the desired number of layers\n","to freeze the model_init_frozen function might have to be adjusted.\n","To see which layers are available run:\n","\n","  for name, param in model.named_parameters():\n","    print(name, param)\n","\n","Observe entire model architecture and note layers you wish to freeze. Adjust\n","*conditional statement accordingly.\n","\n","# https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751\n","\"\"\"\n","def model_init_frozen():\n","  model = AutoModelForSequenceClassification.from_pretrained(_base_model, num_labels=2, return_dict=True)\n","  for name, param in model.named_parameters():\n","    # *conditional statement: currently all encoder layers are frozen\n","    if \".layer.\" in name:\n","      param.requires_grad = False\n","  return model\n","\n","def model_init():\n","  return AutoModelForSequenceClassification.from_pretrained(_base_model, num_labels=2, return_dict=True)\n","\n","model_inits = {\"unfrozen\": model_init, \"frozen\": model_init_frozen}"],"metadata":{"id":"wibKu7-clCxD","executionInfo":{"status":"ok","timestamp":1703268282908,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation metrics"],"metadata":{"id":"Y_GKy0VSadKm"}},{"cell_type":"code","source":["\"\"\"\n","Below we specify which performance measures we wish to observe during training\n","at the end of each step/epoch.\n","\"\"\"\n","\n","clf_metrics = evaluate.combine(_eval_metrics)\n","\n","def compute_metrics(eval_preds):\n","  logits, labels = eval_preds\n","  predictions = np.argmax(logits, axis=-1)\n","  return clf_metrics.compute(predictions = predictions, references = labels)"],"metadata":{"id":"-8lzwBpt8xoV","executionInfo":{"status":"ok","timestamp":1703268291221,"user_tz":-60,"elapsed":7831,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# CustomTrainer\n","\n","Note: When using one of the soft cost functions (F1 or MCC) we observe slower\n","convergence during training. This might require longer training times.\n","\n","Note: This template in its current form only works with a binary classification task, but can easily be amended to work with multi-class classification tasks."],"metadata":{"id":"s642VW0_MAWs"}},{"cell_type":"code","source":["class CustomTrainer(Trainer):\n","  \"\"\"\n","  Here we define child-class inheriting the behaviour of Trainer. This allows us\n","  to overwrite the custom loss function.\n","  \"\"\"\n","\n","  def __init__(self, type_loss, **kwargs):\n","    # Instantiate Parent Class\n","    super().__init__(**kwargs)\n","    # Assign ChildClass Attributes\n","    self.type_loss = type_loss\n","    self.loss_fcts = {\"wce\": self.weighted_cross_entropy, \"f1\": self.macro_double_soft_f1, \"mcc\": self.mcc_exp, \"ce\":super().compute_loss}\n","\n","  def compute_loss(self, model, inputs, return_outputs=False):\n","    \"\"\"\n","    Overwrite parent's compute_loss, this function will return the desired loss\n","    function specified during initialization of class.\n","    \"\"\"\n","    return self.loss_fcts[self.type_loss](model, inputs, return_outputs)\n","\n","  def macro_double_soft_f1(self, model, inputs, return_outputs=False):\n","\n","      \"\"\"\n","      Compute the macro soft F1-score as a cost (average 1 - soft-F1 across\n","      all labels).\n","      Use probability values instead of binary predictions.\n","\n","      https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d\n","      https://arxiv.org/abs/2108.10566\n","      https://www.kaggle.com/code/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n","      \"\"\"\n","\n","      # prepare inputs\n","      y = inputs.pop(\"labels\")\n","      outputs = model(**inputs)\n","      logits = outputs.get(\"logits\")\n","      y_hat = torch.nn.functional.softmax(logits, dim=1)\n","      # construct soft scores\n","      tp = (y_hat[:, 1] * y).sum(dim=0)\n","      fn = (y_hat[:, 0] * y).sum(dim=0)\n","      fp = (y_hat[:, 1] * (1-y)).sum(dim=0)\n","      tn = (y_hat[:, 0] * (1-y)).sum(dim=0)\n","      # calculate cost\n","      soft_f1_class1 = 2*tp / (2*tp + fn + fp + 1e-16)\n","      soft_f1_class0 = 2*tn / (2*tn + fn + fp + 1e-16)\n","      cost_class1 = 1 - soft_f1_class1\n","      cost_class0 = 1 - soft_f1_class0 # reduce 1 - f1 to maximize f1\n","      cost = 0.5 * (cost_class1 + cost_class0) # take into account both class 1 and class 0\n","      # compute average\n","      loss = cost.mean()\n","      return (loss, outputs) if return_outputs else loss\n","\n","  def weighted_cross_entropy(self, model, inputs, return_outputs=False):\n","    \"\"\"\n","    This method employs standard Cross-Entropy but puts different weights on the\n","    classes.\n","    With this, should one class be of more importance we can overweigh its\n","    impact on the loss, thereby indirectly penalizing the other class.\n","    \"\"\"\n","    labels = inputs.pop(\"labels\")\n","    # forward pass\n","    outputs = model(**inputs)\n","    logits = outputs.get(\"logits\")\n","    # compute loss - adjust weights for classes as desired\n","    loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0], device=model.device))\n","    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","    return (loss, outputs) if return_outputs else loss\n","\n","  def mcc_exp(self, model, inputs, return_outputs=False):\n","    \"\"\"\n","    Computes a sort of soft MCC score, similiarly to the soft-F1, by using\n","    probability measures instead of binary predictions.\n","\n","    https://www.kaggle.com/code/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n","    https://github.com/vlainic/matthews-correlation-coefficient/tree/master\n","    https://arxiv.org/abs/2010.13454\n","    \"\"\"\n","    # prepare inputs\n","    y = inputs.pop(\"labels\")\n","    outputs = model(**inputs)\n","    logits = outputs.get(\"logits\")\n","    y_hat = torch.nn.functional.softmax(logits, dim=1)\n","    # construct soft scores\n","    tp = (y_hat[:, 1] * y).sum(dim=0)\n","    fn = (y_hat[:, 0] * y).sum(dim=0)\n","    fp = (y_hat[:, 1] * (1-y)).sum(dim=0)\n","    tn = (y_hat[:, 0] * (1-y)).sum(dim=0)\n","  # calculate cost\n","    mcc = (tn * tp - fn * fp)/ torch.sqrt(((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn))+ 1e-16)\n","    loss_mcc = 1 - mcc\n","\n","    return (loss_mcc, outputs) if return_outputs else loss_mcc"],"metadata":{"id":"qm6grCG3lnhl","executionInfo":{"status":"ok","timestamp":1703268292863,"user_tz":-60,"elapsed":1646,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Initialize CustomTrainer"],"metadata":{"id":"YvjtaX67SAxi"}},{"cell_type":"code","source":["trainer = CustomTrainer(\n","    type_loss = _loss_fct,\n","    model_init = model_inits[_frozen],\n","    args = training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics = compute_metrics,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvWoN2ipR71V","executionInfo":{"status":"ok","timestamp":1703268296418,"user_tz":-60,"elapsed":3557,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}},"outputId":"531ba169-74f9-4608-a304-3e4ce529ae9d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["# (Optional) Create and assign an Optimizer and Scheduler\n","\n","When using the HuggingFace Trainer API for hyperparameter search, we can no longer use the \"optimizer\" argument directly. Instead we customize the optimizer and scheduler\n","\n","Note: This is rather optional, as we could skip the following step and use the defaults. Inclusion in case some custom behaviour is desired."],"metadata":{"id":"jJ7ygag4SIFV"}},{"cell_type":"code","source":["\n","\"\"\"\n","When using the HugginFace Trainer API for hyperparameter search, we can no longer use\n","the \"optimizer\" argument directly. Instead we customize the optimizer and scheduler\n","\"\"\"\n","optimizer = torch.optim.AdamW(trainer.model.parameters())\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer = optimizer,\n","    num_warmup_steps = 0,\n","    num_training_steps = training_args.num_train_epochs * tokenized_datasets[\"train\"].num_rows\n","\n",")\n","\n","# Uncomment line below if you wish to pass objects to Trainer\n","\"\"\"\n","Pass instances to Trainer\n","\"\"\"\n","#trainer.optimizers = (optimizer, lr_scheduler)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"DjaTybKZR75P","executionInfo":{"status":"ok","timestamp":1703268296419,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}},"outputId":"8524c1df-d0e5-42b1-fb24-66bd38359a99"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nPass instances to Trainer\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Hyperparameter Search via Optuna\n","\n","Adjust hyperparameters and their ranges as desired\n","\n","\n","Note: warmup_ratio fulfills a somewhat similar role to freezing. It is also often used to stabilize training at the beginning and avoid large weight updates.\n","\n","https://towardsdatascience.com/state-of-the-art-machine-learning-hyperparameter-optimization-with-optuna-a315d8564de1\n","\n","https://huggingface.co/docs/transformers/hpo_train\n","\n","https://github.com/bayesian-optimization/BayesianOptimization\n","\n"],"metadata":{"id":"LzY5_Rkj5DsG"}},{"cell_type":"code","source":["# Define objective function that later selects best model based upon specific metric\n","def compute_objective(metrics):\n","  return metrics[_metric_best_model]\n","\n","# Define search space for hyperparamter tuning\n","def optuna_hp_space(trial):\n","  return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n","        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n","        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 8),\n","        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-1),\n","        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0, 1e-1),\n","    }"],"metadata":{"id":"otwC5-wurHsq","executionInfo":{"status":"ok","timestamp":1703268296419,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ilias Matthias Nasri","userId":"17108654071466836904"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Run Hyperparameter Search"],"metadata":{"id":"O7ILtPjkStWa"}},{"cell_type":"code","source":["# Run hyperparameter search\n","best_run = trainer.hyperparameter_search(\n","    direction=\"maximize\",\n","    backend=\"optuna\",\n","    hp_space = optuna_hp_space,\n","    n_trials = _no_trials,\n","    compute_objective = compute_objective\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"nk8zx24BStiR","outputId":"e6dba2ad-116d-4124-ba98-25a6f74d1b81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2023-12-22 18:04:56,681] A new study created in memory with name: no-name-fad3d733-5584-4ee0-adef-dac87629ea35\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1380/1380 06:45, Epoch 6/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.953700</td>\n","      <td>0.861584</td>\n","      <td>0.659314</td>\n","      <td>0.843137</td>\n","      <td>0.616487</td>\n","      <td>0.712215</td>\n","      <td>0.342623</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.780400</td>\n","      <td>0.732474</td>\n","      <td>0.737745</td>\n","      <td>0.835938</td>\n","      <td>0.767025</td>\n","      <td>0.800000</td>\n","      <td>0.424552</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.638400</td>\n","      <td>0.642787</td>\n","      <td>0.759804</td>\n","      <td>0.849421</td>\n","      <td>0.788530</td>\n","      <td>0.817844</td>\n","      <td>0.469541</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.506600</td>\n","      <td>0.594873</td>\n","      <td>0.803922</td>\n","      <td>0.813880</td>\n","      <td>0.924731</td>\n","      <td>0.865772</td>\n","      <td>0.522041</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.442000</td>\n","      <td>0.544282</td>\n","      <td>0.811275</td>\n","      <td>0.836667</td>\n","      <td>0.899642</td>\n","      <td>0.867012</td>\n","      <td>0.547845</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.402700</td>\n","      <td>0.539498</td>\n","      <td>0.811275</td>\n","      <td>0.834437</td>\n","      <td>0.903226</td>\n","      <td>0.867470</td>\n","      <td>0.546736</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2023-12-22 18:11:46,742] Trial 0 finished with value: 0.5467361742360426 and parameters: {'learning_rate': 4.042625645902345e-06, 'per_device_train_batch_size': 16, 'num_train_epochs': 6, 'weight_decay': 0.046663145203696514, 'warmup_ratio': 0.05677302733685228}. Best is trial 0 with value: 0.5467361742360426.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='150' max='232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [150/232 02:35 < 01:26, 0.95 it/s, Epoch 2.57/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.986100</td>\n","      <td>0.971853</td>\n","      <td>0.627451</td>\n","      <td>0.782222</td>\n","      <td>0.630824</td>\n","      <td>0.698413</td>\n","      <td>0.234649</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.935100</td>\n","      <td>0.922842</td>\n","      <td>0.671569</td>\n","      <td>0.825112</td>\n","      <td>0.659498</td>\n","      <td>0.733068</td>\n","      <td>0.333608</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["# Outputs best hyperparameters that lead to maximizing the objective function\n","best_run"],"metadata":{"id":"HgJwNcvpPf2P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now reinitialize the above classes with the training arguments contained in best_runs and train the model on both the Training and Validation dataset to measure its performance on the test dataset.\n","\n","(Optional) If during initial training you froze some layers you can now continue training with a partially/fully unfrozen model."],"metadata":{"id":"idjhc_d4ZZKw"}},{"cell_type":"code","source":["best_training_args = TrainingArguments(\n","    _path_secondary_training,\n","    **best_runs.hyperparameters,\n","    )"],"metadata":{"id":"SBOLOMHgam1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# adjust as described above\n","def model_init_secondary_run():\n","  return AutoModelForSequenceClassification.from_pretrained(_base_model, num_labels=2, return_dict=True)\n"],"metadata":{"id":"TZGrvAxfuoBF"},"execution_count":null,"outputs":[]}]}