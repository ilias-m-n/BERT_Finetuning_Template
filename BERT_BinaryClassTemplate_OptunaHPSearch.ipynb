{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoiMafe4IZH2"
   },
   "source": [
    "# Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14205,
     "status": "ok",
     "timestamp": 1703268266732,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "BzaTHUtWcb4T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import evaluate\n",
    "from transformers import (\n",
    "     AutoTokenizer,\n",
    "     DataCollatorWithPadding,\n",
    "     TrainingArguments,\n",
    "     AutoModelForSequenceClassification,\n",
    "     Trainer,\n",
    "     logging,\n",
    "     AdamW,\n",
    "     get_scheduler,\n",
    "\n",
    ")\n",
    "import torch\n",
    "from ray import tune, train\n",
    "\n",
    "# turn off warnings\n",
    "#logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrZ91MToIwEf"
   },
   "source": [
    "# Meta Variables\n",
    "- base model\n",
    "- loss function\n",
    "- evaluation metrics\n",
    "- best model metric\n",
    "- number of trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1703268266732,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "iGkjtCJ6yAgG"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Directory Paths:\n",
    "\"\"\"\n",
    "_path_initial_training = \"./initial_training\"\n",
    "_path_secondary_training = \"./secondary_training\"\n",
    "\n",
    "\"\"\"\n",
    "Base BERT model to be used during finetuning.\n",
    "This has to be picked from the pre-trained models on HuggingFace\n",
    "in order to be compatible with the Trainer API\n",
    "\"\"\"\n",
    "_base_model = \"bert-base-uncased\"\n",
    "\n",
    "\"\"\"\n",
    "Three custom loss functions have been implemented:\n",
    "  f1: soft-f1 macro score\n",
    "  mcc: soft-mcc\n",
    "  wce: weighted cross entropy\n",
    "  ce: standard cross entropy\n",
    "\"\"\"\n",
    "_loss_fct = \"mcc\"\n",
    "\n",
    "\"\"\"\n",
    "Metrics listed during evaluation:\n",
    "\n",
    "Note: adjust with desired metrics.\n",
    "\"\"\"\n",
    "_eval_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"matthews_correlation\"]\n",
    "\n",
    "\"\"\"\n",
    "Specify which metric should be maximized during hyperparameter-search\n",
    "Options:\n",
    "- eval_matthews_correlation\n",
    "- eval_f1\n",
    "- eval_loss\n",
    "- any other metric passed to the compute_metrics function\n",
    "\"\"\"\n",
    "_metric_best_model = \"eval_matthews_correlation\"\n",
    "\n",
    "\"\"\"\n",
    "Number of trials to run during hyperparameter search.\n",
    "\"\"\"\n",
    "_no_trials = 4\n",
    "\n",
    "\"\"\"\n",
    "Employ freezing of layers, options:\n",
    "\"unfrozen\": all layers unfrozen\n",
    "\"frozen\": all transformer layers frozen\n",
    "\"\"\"\n",
    "_frozen = \"unfrozen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYAAghnjIzBt"
   },
   "source": [
    "# Setup\n",
    "\n",
    "This part has to be adjusted to whatever dataset and format used.\n",
    "\n",
    "Note: DataCollatorWithPadding allows for dynamic padding for individual batches. Only use with GPUs. For TPUs, use max_length padding attribute with Tokenizer instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVotGcMsQLKC"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "-Note: We use GLUE's Microsoft Research Paraphrase Corpus for testing the functionality of this template\n",
    "\n",
    "https://huggingface.co/datasets/viewer/?dataset=glue&config=mrpc\n",
    "\n",
    "Binary Classification Task:\n",
    "MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1703268268360,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "dnPxVcJqQLSC"
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY2CaIW5QCY3"
   },
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1703268268723,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "w7qbphxLQAia"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(_base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MUIwuMIQ0Hx"
   },
   "source": [
    "## Function that returns the Tokenizer so that we can employ data mapping.\n",
    "\n",
    "Note: Adjust this to desired task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1703268268724,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "6F83g3QqQAo3"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwQnmO5sRKgG"
   },
   "source": [
    "## Map Dataset with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1703268268724,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "QKBC_DDhc0sY"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvgXUeGJRZEg"
   },
   "source": [
    "## Instantiate DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1703268268724,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "LWJoWdWJRZMe"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJWBvuj_JL8y"
   },
   "source": [
    "# Training Arguments\n",
    "\n",
    "Adjust to desired behaviour. Most arguments can be learned during hyperparameter-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703268282181,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "O9t82JVzjz4j"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create instance of class TrainingArguments. Adjust to desired behaviour.\n",
    "\"\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = _path_initial_training,\n",
    "    # This was set for testing, when using template I would recommend adding\n",
    "    # some sort of datatime argument to above meta path variables, to not loose\n",
    "    # previous learnings.\n",
    "    overwrite_output_dir = True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    metric_for_best_model = _metric_best_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjtjOcw3JQ-E"
   },
   "source": [
    "# Model Initialzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1703268282908,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "wibKu7-clCxD"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we supply two model init functions, one that freezes all encoder layers and\n",
    "one that does not.\n",
    "\n",
    "Pass desired init function to Trainer below.\n",
    "\n",
    "Gradual unfreezing helps to strike a balance between leveraging pre-trained\n",
    "knowledge and adapting to task-specific data. By unfreezing layers gradually\n",
    "during training, the model learns to prioritize retaining general linguistic\n",
    "knowledge in the early layers while fine-tuning the higher layers to adapt to\n",
    "task-specific nuances. This mitigates overfitting by allowing the model to\n",
    "gradually specialize on the new task without abruptly forgetting the\n",
    "linguistic representations learned during pre-training, resulting in more\n",
    "effective adaptation and improved generalization to the target task.\n",
    "\n",
    "Note: When utilizing gradual unfreezing you will have to train the model in\n",
    "multiple steps. Gradually unfreezing ever more layers during training.\n",
    "You will observe slower convergence, as such this will take more time.\n",
    "\n",
    "Note: Depending on the choice of a base model and the desired number of layers\n",
    "to freeze the model_init_frozen function might have to be adjusted.\n",
    "To see which layers are available run:\n",
    "\n",
    "  for name, param in model.named_parameters():\n",
    "    print(name, param)\n",
    "\n",
    "Observe entire model architecture and note layers you wish to freeze. Adjust\n",
    "*conditional statement accordingly.\n",
    "\n",
    "# https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751\n",
    "\"\"\"\n",
    "def model_init_frozen():\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(_base_model, num_labels=2, return_dict=True)\n",
    "  for name, param in model.named_parameters():\n",
    "    # *conditional statement: currently all encoder layers are frozen\n",
    "    if \".layer.\" in name:\n",
    "      param.requires_grad = False\n",
    "  return model\n",
    "\n",
    "def model_init():\n",
    "  return AutoModelForSequenceClassification.from_pretrained(_base_model, num_labels=2, return_dict=True)\n",
    "\n",
    "model_inits = {\"unfrozen\": model_init, \"frozen\": model_init_frozen}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_GKy0VSadKm"
   },
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7831,
     "status": "ok",
     "timestamp": 1703268291221,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "-8lzwBpt8xoV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below we specify which performance measures we wish to observe during training\n",
    "at the end of each step/epoch.\n",
    "\"\"\"\n",
    "\n",
    "clf_metrics = evaluate.combine(_eval_metrics)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  logits, labels = eval_preds\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return clf_metrics.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s642VW0_MAWs"
   },
   "source": [
    "# CustomTrainer\n",
    "\n",
    "Note: When using one of the soft cost functions (F1 or MCC) we observe slower\n",
    "convergence during training. This might require longer training times.\n",
    "\n",
    "Note: This template in its current form only works with a binary classification task, but can easily be amended to work with multi-class classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1646,
     "status": "ok",
     "timestamp": 1703268292863,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "qm6grCG3lnhl"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "  \"\"\"\n",
    "  Here we define child-class inheriting the behaviour of Trainer. This allows us\n",
    "  to overwrite the custom loss function.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, type_loss, **kwargs):\n",
    "    # Instantiate Parent Class\n",
    "    super().__init__(**kwargs)\n",
    "    # Assign ChildClass Attributes\n",
    "    self.type_loss = type_loss\n",
    "    self.loss_fcts = {\"wce\": self.weighted_cross_entropy, \"f1\": self.macro_double_soft_f1, \"mcc\": self.mcc_exp, \"ce\":super().compute_loss}\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    \"\"\"\n",
    "    Overwrite parent's compute_loss, this function will return the desired loss\n",
    "    function specified during initialization of class.\n",
    "    \"\"\"\n",
    "    return self.loss_fcts[self.type_loss](model, inputs, return_outputs)\n",
    "\n",
    "  def macro_double_soft_f1(self, model, inputs, return_outputs=False):\n",
    "\n",
    "      \"\"\"\n",
    "      Compute the macro soft F1-score as a cost (average 1 - soft-F1 across\n",
    "      all labels).\n",
    "      Use probability values instead of binary predictions.\n",
    "\n",
    "      https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d\n",
    "      https://arxiv.org/abs/2108.10566\n",
    "      https://www.kaggle.com/code/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "      \"\"\"\n",
    "\n",
    "      # prepare inputs\n",
    "      y = inputs.pop(\"labels\")\n",
    "      outputs = model(**inputs)\n",
    "      logits = outputs.get(\"logits\")\n",
    "      y_hat = torch.nn.functional.softmax(logits, dim=1)\n",
    "      # construct soft scores\n",
    "      tp = (y_hat[:, 1] * y).sum(dim=0)\n",
    "      fn = (y_hat[:, 0] * y).sum(dim=0)\n",
    "      fp = (y_hat[:, 1] * (1-y)).sum(dim=0)\n",
    "      tn = (y_hat[:, 0] * (1-y)).sum(dim=0)\n",
    "      # calculate cost\n",
    "      soft_f1_class1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "      soft_f1_class0 = 2*tn / (2*tn + fn + fp + 1e-16)\n",
    "      cost_class1 = 1 - soft_f1_class1\n",
    "      cost_class0 = 1 - soft_f1_class0 # reduce 1 - f1 to maximize f1\n",
    "      cost = 0.5 * (cost_class1 + cost_class0) # take into account both class 1 and class 0\n",
    "      # compute average\n",
    "      loss = cost.mean()\n",
    "      return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "  def weighted_cross_entropy(self, model, inputs, return_outputs=False):\n",
    "    \"\"\"\n",
    "    This method employs standard Cross-Entropy but puts different weights on the\n",
    "    classes.\n",
    "    With this, should one class be of more importance we can overweigh its\n",
    "    impact on the loss, thereby indirectly penalizing the other class.\n",
    "    \"\"\"\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    # forward pass\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.get(\"logits\")\n",
    "    # compute loss - adjust weights for classes as desired\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0], device=model.device))\n",
    "    loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "  def mcc_exp(self, model, inputs, return_outputs=False):\n",
    "    \"\"\"\n",
    "    Computes a sort of soft MCC score, similiarly to the soft-F1, by using\n",
    "    probability measures instead of binary predictions.\n",
    "\n",
    "    https://www.kaggle.com/code/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n",
    "    https://github.com/vlainic/matthews-correlation-coefficient/tree/master\n",
    "    https://arxiv.org/abs/2010.13454\n",
    "    \"\"\"\n",
    "    # prepare inputs\n",
    "    y = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.get(\"logits\")\n",
    "    y_hat = torch.nn.functional.softmax(logits, dim=1)\n",
    "    # construct soft scores\n",
    "    tp = (y_hat[:, 1] * y).sum(dim=0)\n",
    "    fn = (y_hat[:, 0] * y).sum(dim=0)\n",
    "    fp = (y_hat[:, 1] * (1-y)).sum(dim=0)\n",
    "    tn = (y_hat[:, 0] * (1-y)).sum(dim=0)\n",
    "  # calculate cost\n",
    "    mcc = (tn * tp - fn * fp)/ torch.sqrt(((tp + fp)*(tp + fn)*(tn + fp)*(tn + fn))+ 1e-16)\n",
    "    loss_mcc = 1 - mcc\n",
    "\n",
    "    return (loss_mcc, outputs) if return_outputs else loss_mcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvjtaX67SAxi"
   },
   "source": [
    "# Initialize CustomTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3557,
     "status": "ok",
     "timestamp": 1703268296418,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "YvWoN2ipR71V",
    "outputId": "531ba169-74f9-4608-a304-3e4ce529ae9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    type_loss = _loss_fct,\n",
    "    model_init = model_inits[_frozen],\n",
    "    args = training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ7ygag4SIFV"
   },
   "source": [
    "# (Optional) Create and assign an Optimizer and Scheduler\n",
    "\n",
    "When using the HuggingFace Trainer API for hyperparameter search, we can no longer use the \"optimizer\" argument directly. Instead we customize the optimizer and scheduler\n",
    "\n",
    "Note: This is rather optional, as we could skip the following step and use the defaults. Inclusion in case some custom behaviour is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1703268296419,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "DjaTybKZR75P",
    "outputId": "8524c1df-d0e5-42b1-fb24-66bd38359a99"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nPass instances to Trainer\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "When using the HugginFace Trainer API for hyperparameter search, we can no longer use\n",
    "the \"optimizer\" argument directly. Instead we customize the optimizer and scheduler\n",
    "\"\"\"\n",
    "optimizer = torch.optim.AdamW(trainer.model.parameters())\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = training_args.num_train_epochs * tokenized_datasets[\"train\"].num_rows\n",
    "\n",
    ")\n",
    "\n",
    "# Uncomment line below if you wish to pass objects to Trainer\n",
    "\"\"\"\n",
    "Pass instances to Trainer\n",
    "\"\"\"\n",
    "#trainer.optimizers = (optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzY5_Rkj5DsG"
   },
   "source": [
    "# Hyperparameter Search via Optuna\n",
    "\n",
    "Adjust hyperparameters and their ranges as desired\n",
    "\n",
    "\n",
    "Note: warmup_ratio fulfills a somewhat similar role to freezing. It is also often used to stabilize training at the beginning and avoid large weight updates.\n",
    "\n",
    "https://towardsdatascience.com/state-of-the-art-machine-learning-hyperparameter-optimization-with-optuna-a315d8564de1\n",
    "\n",
    "https://huggingface.co/docs/transformers/hpo_train\n",
    "\n",
    "https://github.com/bayesian-optimization/BayesianOptimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1703268296419,
     "user": {
      "displayName": "Ilias Matthias Nasri",
      "userId": "17108654071466836904"
     },
     "user_tz": -60
    },
    "id": "otwC5-wurHsq"
   },
   "outputs": [],
   "source": [
    "# Define objective function that later selects best model based upon specific metric\n",
    "def compute_objective(metrics):\n",
    "  return metrics[_metric_best_model]\n",
    "\n",
    "# Define search space for hyperparamter tuning\n",
    "def optuna_hp_space(trial):\n",
    "  return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 8),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-1),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0, 1e-1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7ILtPjkStWa"
   },
   "source": [
    "# Run Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "nk8zx24BStiR",
    "outputId": "e6dba2ad-116d-4124-ba98-25a6f74d1b81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-22 18:04:56,681] A new study created in memory with name: no-name-fad3d733-5584-4ee0-adef-dac87629ea35\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 06:45, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.953700</td>\n",
       "      <td>0.861584</td>\n",
       "      <td>0.659314</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.616487</td>\n",
       "      <td>0.712215</td>\n",
       "      <td>0.342623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.780400</td>\n",
       "      <td>0.732474</td>\n",
       "      <td>0.737745</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.767025</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.424552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.638400</td>\n",
       "      <td>0.642787</td>\n",
       "      <td>0.759804</td>\n",
       "      <td>0.849421</td>\n",
       "      <td>0.788530</td>\n",
       "      <td>0.817844</td>\n",
       "      <td>0.469541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.506600</td>\n",
       "      <td>0.594873</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.813880</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.865772</td>\n",
       "      <td>0.522041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>0.544282</td>\n",
       "      <td>0.811275</td>\n",
       "      <td>0.836667</td>\n",
       "      <td>0.899642</td>\n",
       "      <td>0.867012</td>\n",
       "      <td>0.547845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.402700</td>\n",
       "      <td>0.539498</td>\n",
       "      <td>0.811275</td>\n",
       "      <td>0.834437</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.546736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-22 18:11:46,742] Trial 0 finished with value: 0.5467361742360426 and parameters: {'learning_rate': 4.042625645902345e-06, 'per_device_train_batch_size': 16, 'num_train_epochs': 6, 'weight_decay': 0.046663145203696514, 'warmup_ratio': 0.05677302733685228}. Best is trial 0 with value: 0.5467361742360426.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/232 02:35 < 01:26, 0.95 it/s, Epoch 2.57/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.986100</td>\n",
       "      <td>0.971853</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.782222</td>\n",
       "      <td>0.630824</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.234649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.935100</td>\n",
       "      <td>0.922842</td>\n",
       "      <td>0.671569</td>\n",
       "      <td>0.825112</td>\n",
       "      <td>0.659498</td>\n",
       "      <td>0.733068</td>\n",
       "      <td>0.333608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run hyperparameter search\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space = optuna_hp_space,\n",
    "    n_trials = _no_trials,\n",
    "    compute_objective = compute_objective\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgJwNcvpPf2P"
   },
   "outputs": [],
   "source": [
    "# Outputs best hyperparameters that lead to maximizing the objective function\n",
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idjhc_d4ZZKw"
   },
   "source": [
    "We can now reinitialize the above classes with the training arguments contained in best_runs and train the model on both the Training and Validation dataset to measure its performance on the test dataset.\n",
    "\n",
    "(Optional) If during initial training you froze some layers you can now continue training with a partially/fully unfrozen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBOLOMHgam1C"
   },
   "outputs": [],
   "source": [
    "best_training_args = TrainingArguments(\n",
    "    _path_secondary_training,\n",
    "    **best_runs.hyperparameters,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZGrvAxfuoBF"
   },
   "outputs": [],
   "source": [
    "# adjust as described above\n",
    "def model_init_secondary_run():\n",
    "  return AutoModelForSequenceClassification.from_pretrained(_base_model, num_labels=2, return_dict=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP3hjEY+RACi/Hod/unK24C",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
